{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ztmjC6846JXO",
        "NCM7vK1tgmyS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Init"
      ],
      "metadata": {
        "id": "ztmjC6846JXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Might Remove this, who knows? not me!\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "NQt-GsRiyULb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helpers"
      ],
      "metadata": {
        "id": "69LzSdwFapYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_maze_from_txt(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # First line contains dimensions (optional if you want to use it)\n",
        "    rows, cols = map(int, lines[0].strip().split())\n",
        "\n",
        "    # The rest of the lines contain the maze grid\n",
        "    maze = []\n",
        "    for line in lines[1:]:\n",
        "        row = list(map(int, line.strip().split()))\n",
        "        maze.append(row)\n",
        "\n",
        "    return maze, rows, cols"
      ],
      "metadata": {
        "id": "oF5E2NV56Sa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent_dqn(agent, config):\n",
        "    maze, rows, cols = load_maze_from_txt(f'maze_{config.maze_size}.txt')\n",
        "    env = MazeEnv(maze_matrix=maze, max_steps=config.max_steps, move_prob=config.move_prob, rewards=config.rewards)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    print(\"\\nInitial maze state:\")\n",
        "    env.render()\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.99995\n",
        "    reached_goal = False\n",
        "    while not done:\n",
        "      action, epsilon = agent.get_action(state, epsilon, epsilon_decay)\n",
        "      next_state, reward, done, reached_goal = env.step(action)\n",
        "      state = next_state\n",
        "      total_reward += reward\n",
        "    env.render()\n",
        "    print(f\"Final maze state (Total reward: {total_reward:.2f})\")\n",
        "    print(f\"Reached Goal: {reached_goal}\")"
      ],
      "metadata": {
        "id": "eL8nd89NoL4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards(reward_history, maze_size, agent_type=\"dqn\"):\n",
        "    reward_history = list(map(lambda x: x[0], reward_history))\n",
        "    episodes = len(reward_history)\n",
        "    window = max(5, episodes // 4)  # 5% of total episodes, but at least 5\n",
        "\n",
        "    moving_avg = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(reward_history, alpha=0.4, label=\"Episode Reward\")\n",
        "    plt.plot(range(window - 1, episodes), moving_avg, label=f\"Moving Avg (window={window})\", linewidth=2)\n",
        "\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(f\"Reward Progression on Maze {maze_size} ({agent_type.upper()})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sb2m1occs1Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent_from_tuples(episode_rewards, maze_size):\n",
        "    reward_values = list(map(lambda x: x[0], episode_rewards))\n",
        "    success_flags = list(map(lambda x: x[1], episode_rewards))\n",
        "\n",
        "    reward_values = np.array(reward_values)\n",
        "    success_flags = np.array(success_flags)\n",
        "\n",
        "    stats = {\n",
        "        'episodes': len(reward_values),\n",
        "        'mean_reward': np.mean(reward_values),\n",
        "        'median_reward': np.median(reward_values),\n",
        "        'min_reward': np.min(reward_values),\n",
        "        'max_reward': np.max(reward_values),\n",
        "        'std_reward': np.std(reward_values),\n",
        "        'success_rate': np.mean(success_flags) * 100  # percent\n",
        "    }\n",
        "\n",
        "    print(f\"Maze Size {maze_size} Agent Evaluation Stats:\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\"{k}: {v:.2f}\" if isinstance(v, float) else f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "R81Uin1DBWZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Maze Environment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FNcDAB90faxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2iSrtkzfW8Z"
      },
      "outputs": [],
      "source": [
        "class MazeEnv:\n",
        "    def __init__(self, maze_matrix, max_steps=300, move_prob=0.99, rewards=[1,-75,-5,1000,1,-.5,-2]): # Config for maze 11\n",
        "        self.original_maze = np.array(maze_matrix)\n",
        "        self.maze = self.original_maze.copy()\n",
        "        self.max_steps = max_steps\n",
        "        self.move_prob = move_prob\n",
        "        self.agent_pos = None\n",
        "        self.goal_pos = tuple(map(int, np.argwhere(self.maze == 4)[0]))  # 4 = goal\n",
        "        self.start_pos = tuple(map(int, np.argwhere(self.maze == 3)[0])) # 3 = start\n",
        "        self.visit_counts = np.zeros_like(self.maze)\n",
        "        self.reset()\n",
        "        self.rewards = rewards\n",
        "        self.prev_dist = self.manhattan_dist(self.agent_pos, self.goal_pos)\n",
        "\n",
        "    def reset(self):\n",
        "        self.maze = self.original_maze.copy()\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.steps = 0\n",
        "        self.prev_dist = self.manhattan_dist(self.agent_pos, self.goal_pos)\n",
        "        self.visit_counts = np.zeros_like(self.maze)\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        # Option: flattened maze + one-hot agent position\n",
        "        flat_maze = self.maze.flatten()\n",
        "        agent_pos_vec = np.zeros(flat_maze.shape)\n",
        "        idx = self.agent_pos[0] * self.maze.shape[1] + self.agent_pos[1]\n",
        "        agent_pos_vec[idx] = 1\n",
        "        return np.concatenate([flat_maze, agent_pos_vec])\n",
        "\n",
        "    def is_valid(self, pos):\n",
        "        x, y = pos\n",
        "        return (0 <= x < self.maze.shape[0]) and (0 <= y < self.maze.shape[1]) and (self.maze[x, y] != 1)\n",
        "\n",
        "    def manhattan_dist(self, pos1, pos2):\n",
        "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "\n",
        "        move_map = {\n",
        "            0: (-1, 0),   # up\n",
        "            1: (1, 0),    # down\n",
        "            2: (0, -1),   # left\n",
        "            3: (0, 1),    # right\n",
        "        }\n",
        "\n",
        "        # Stochasticity\n",
        "        if random.random() > self.move_prob:\n",
        "            action = random.choice([0, 1, 2, 3])  # Random move\n",
        "\n",
        "        dx, dy = move_map[action]\n",
        "        new_pos = (self.agent_pos[0] + dx, self.agent_pos[1] + dy)\n",
        "\n",
        "        hit_the_wall = True\n",
        "        if self.is_valid(new_pos):\n",
        "            hit_the_wall = False\n",
        "            self.agent_pos = new_pos  # move\n",
        "\n",
        "        reward = self.rewards[0]\n",
        "        if hit_the_wall:\n",
        "            reward = self.rewards[0]*self.rewards[2]  # default cost for hitting wall\n",
        "        else:\n",
        "            reward = self.rewards[0]  # default step cost\n",
        "            # Directional shaping reward\n",
        "            curr_dist = self.manhattan_dist(self.agent_pos, self.goal_pos)\n",
        "            if curr_dist < self.prev_dist:\n",
        "                reward += self.rewards[4]  # small bonus for progress\n",
        "            elif curr_dist > self.prev_dist:\n",
        "                reward += self.rewards[5]  # optional: small penalty for going wrong direction\n",
        "            self.prev_dist = curr_dist\n",
        "\n",
        "            # Visitation penalty\n",
        "            self.visit_counts[self.agent_pos] += 1\n",
        "            if self.visit_counts[self.agent_pos] > 3:\n",
        "                reward += self.rewards[6]  # discourage revisiting same cell too often\n",
        "\n",
        "        # reward = self.rewards[0]\n",
        "        done = False\n",
        "        reached_goal = False\n",
        "\n",
        "        cell_value = self.maze[self.agent_pos]\n",
        "        if cell_value == 2:  # trap\n",
        "            reward = self.rewards[2]\n",
        "            done = True\n",
        "        elif self.agent_pos == self.goal_pos: # Reached goal\n",
        "            reward = self.rewards[3]\n",
        "            reached_goal = True\n",
        "            done = True\n",
        "        elif self.steps >= self.max_steps:\n",
        "            reward = self.rewards[1]  # Penalty for timeout\n",
        "            done = True\n",
        "\n",
        "        return self.get_state(), reward, done, reached_goal\n",
        "\n",
        "    def render(self):\n",
        "        display = self.maze.copy().astype(str)\n",
        "        display[display == '0'] = '.'  # Free\n",
        "        display[display == '1'] = '#'  # Wall\n",
        "        display[display == '2'] = 'X'  # Trap\n",
        "        display[display == '3'] = 'S'  # Start\n",
        "        display[display == '4'] = 'G'  # Goal\n",
        "        display[display == '9'] = 'P'  # Padding\n",
        "        x, y = self.agent_pos\n",
        "        display[x, y] = 'A'\n",
        "        print('\\n'.join(' '.join(row) for row in display))\n",
        "        print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DQN"
      ],
      "metadata": {
        "id": "NCM7vK1tgmyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DQN Agent"
      ],
      "metadata": {
        "id": "HeQcPan_a09C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, action_size))\n",
        "    self.action_size = action_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "NE6EXD9t4yeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size=5, hidden_size=128,\n",
        "                 buffer_size=10000, batch_size=64, gamma=0.99, lr=1e-3,\n",
        "                 target_update = 100,  learn_frequency = 2,\n",
        "                 start_training = 1000):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Q-Networks\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, hidden_size).cuda()\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, hidden_size).cuda()\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        # Initialize optimizer with gradient clipping\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
        "\n",
        "        # Replay buffer - use numpy arrays for efficiency\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.global_step = 0\n",
        "        self.target_update = target_update\n",
        "        self.start_training = start_training\n",
        "        self.learn_frequency = learn_frequency\n",
        "\n",
        "\n",
        "    def get_action(self, state, epsilon, epsilon_decay):\n",
        "        \"\"\"Returns action based on epsilon-greedy policy\"\"\"\n",
        "        if random.random() < epsilon:\n",
        "          # perform random action\n",
        "          action = random.randint(0,self.qnetwork_local.action_size-1)\n",
        "        else:\n",
        "          # perform greedy action\n",
        "          with torch.no_grad():\n",
        "            state = torch.from_numpy(state).float().to(self.device)\n",
        "            action = self.qnetwork_local(state).argmax().item()\n",
        "\n",
        "        return action, epsilon * epsilon_decay\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience to replay memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        self.global_step += 1\n",
        "\n",
        "        # Learn if enough samples are available\n",
        "        if self.global_step > self.start_training and self.global_step % self.learn_frequency == 0:\n",
        "            experiences = random.sample(self.memory, self.batch_size)\n",
        "            self.learn(experiences)\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        \"\"\"Update value parameters using batch of experience tuples\"\"\"\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "        states = torch.from_numpy(np.concatenate(states, axis=0)).float().cuda().view(self.batch_size, -1)\n",
        "        actions = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
        "        next_states = torch.from_numpy(np.concatenate(next_states, axis=0)).float().cuda().view(self.batch_size, -1)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float, device=self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float, device=self.device)\n",
        "\n",
        "        # Get network's q value predictions for the chosen states given the selected actions\n",
        "        Q_expected = self.qnetwork_local(states)\n",
        "        Q_sa = Q_expected[torch.arange(states.size(0)), actions]\n",
        "\n",
        "        # Get next state's highest Q values (using target network)\n",
        "        with torch.no_grad():\n",
        "          Q_sa_next = self.qnetwork_target(next_states).max(1)[0]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_sa, rewards + (self.gamma*Q_sa_next*(1-dones)))\n",
        "\n",
        "        # Minimize the loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        if self.global_step % self.target_update == 0:\n",
        "          self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n"
      ],
      "metadata": {
        "id": "2ObAA6_hLvQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train"
      ],
      "metadata": {
        "id": "CdWlF8JnbNEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(config):\n",
        "    print(f\"\\nTraining DQN on Maze {config.maze_size}\")\n",
        "    agent = DQNAgent(\n",
        "        state_size = config.state_size,\n",
        "        action_size=config.action_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        buffer_size=config.buffer_size,\n",
        "        batch_size=config.batch_size,\n",
        "        gamma=config.gamma,\n",
        "        lr=config.lr,\n",
        "        target_update = config.target_update,\n",
        "        learn_frequency = config.learn_frequency,\n",
        "        start_training = config.start_training)\n",
        "\n",
        "    env = config.env\n",
        "    epsilon = config.epsilon\n",
        "    epsilon_decay = config.epsilon_decay\n",
        "    rewards = []\n",
        "\n",
        "    loop = tqdm(total=config.episodes, position=0, leave=False)\n",
        "    for episode in range(config.episodes):\n",
        "        state = env.reset()\n",
        "        done, total_reward, reached_goal = False, 0, False\n",
        "\n",
        "        while not done and total_reward < 53:\n",
        "            action, epsilon = agent.get_action(state, epsilon, epsilon_decay)\n",
        "            next_state, reward, done, reached_goal = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        epsilon *= config.epsilon_decay\n",
        "        rewards.append((total_reward, reached_goal))\n",
        "\n",
        "        loop.set_description(f\"Ep: {episode} R: {total_reward:.2f} Îµ: {epsilon:.3f}\")\n",
        "        loop.update(1)\n",
        "    loop.close()\n",
        "    return agent, rewards\n"
      ],
      "metadata": {
        "id": "uol1z-wBMRqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run and Test"
      ],
      "metadata": {
        "id": "SgtsWblVbTT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNConfig:\n",
        "    def __init__(self, maze_size, env):\n",
        "        self.maze_size = maze_size\n",
        "        self.state_size = 2 * (maze_size ** 2)  # Maze matrix + agent position\n",
        "        self.action_size = 5\n",
        "        self.hidden_size = 128\n",
        "        self.env = env\n",
        "\n",
        "        # Training parameters\n",
        "        self.episodes = 1000\n",
        "        self.gamma = 0.99\n",
        "        self.lr = 1e-3\n",
        "        self.batch_size = 64\n",
        "        self.buffer_size = 10000\n",
        "        self.target_update = 75\n",
        "        self.learn_frequency = 2\n",
        "        self.start_training = 200\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999995\n",
        "        self.max_steps = maze_size ** 2\n",
        "        self.move_prob = 0.99\n",
        "\n",
        "mazes_data = [\n",
        "    {\"size\":5,  \"max_steps\": 75,  \"rewards\":  [1,-10,-5,300,.1,-.1,-5]},  # 0: move, 1: timeout, 2: hit wall, 3: goal, 4: bonus for progress 5: going wrong direction, 6: repeating cell\n",
        "    # {\"size\":7,  \"max_steps\": 100, \"rewards\":  [1,-20,-5,500,.1,-.1,-5]},\n",
        "    # {\"size\":9,  \"max_steps\": 200, \"rewards\":  [1,-35,-5,750,.1,-.1,-5]},\n",
        "]"
      ],
      "metadata": {
        "id": "vzBF2UmWhBf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = get_env_mazes(mazes_data)[0][1]\n",
        "config_DQN = DQNConfig(mazes_data[0][\"size\"], env)\n",
        "agent, reward_history = train_dqn(config_DQN)\n",
        "plot_rewards(reward_history, mazes_data[0][\"size\"])\n",
        "env.play_agent(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bF5ARayKpK",
        "outputId": "1c82b06a-b708-4f71-cc32-3c3936763c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training DQN on 7x7 maze...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial maze state:\n",
            "# # # # # # #\n",
            "# A . . # # #\n",
            "# # # . . # #\n",
            "# . # # . . #\n",
            "# . . # . # #\n",
            "# # . . . G #\n",
            "# # # # # # #\n",
            "\n",
            "# # # # # # #\n",
            "# S . . # # #\n",
            "# # # . . # #\n",
            "# . # # . A #\n",
            "# . . # . # #\n",
            "# # . . . G #\n",
            "# # # # # # #\n",
            "\n",
            "Final maze state (Total reward: -53):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}